{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = '2015-01'\n",
    "sql_transaction = []\n",
    "start_row = 0\n",
    "cleanup = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('{}.db'.format(timeframe))\n",
    "c = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    c.execute(\"CREATE TABLE IF NOT EXISTS parent_reply(parent_id TEXT PRIMARY KEY, comment_id TEXT UNIQUE, parent TEXT, comment TEXT, subreddit TEXT, unix INT, score INT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    data = data.replace('\\n',' newlinechar ').replace('\\r',' newlinechar ').replace('\"',\"'\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_bldr(sql):\n",
    "    global sql_transaction\n",
    "    sql_transaction.append(sql)\n",
    "    if len(sql_transaction) > 1000:\n",
    "        c.execute('BEGIN TRANSACTION')\n",
    "        for s in sql_transaction:\n",
    "            try:\n",
    "                c.execute(s)\n",
    "            except:\n",
    "                pass\n",
    "        connection.commit()\n",
    "        sql_transaction = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_insert_replace_comment(commentid,parentid,parent,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"UPDATE parent_reply SET parent_id = ?, comment_id = ?, parent = ?, comment = ?, subreddit = ?, unix = ?, score = ? WHERE parent_id =?;\"\"\".format(parentid, commentid, parent, comment, subreddit, int(time), score, parentid)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_insert_has_parent(commentid,parentid,parent,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, parent, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parentid, commentid, parent, comment, subreddit, int(time), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_insert_no_parent(commentid,parentid,comment,subreddit,time,score):\n",
    "    try:\n",
    "        sql = \"\"\"INSERT INTO parent_reply (parent_id, comment_id, comment, subreddit, unix, score) VALUES (\"{}\",\"{}\",\"{}\",\"{}\",{},{});\"\"\".format(parentid, commentid, comment, subreddit, int(time), score)\n",
    "        transaction_bldr(sql)\n",
    "    except Exception as e:\n",
    "        print('s0 insertion',str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptable(data):\n",
    "    if len(data.split(' ')) > 1000 or len(data) < 1:\n",
    "        return False\n",
    "    elif len(data) > 32000:\n",
    "        return False\n",
    "    elif data == '[deleted]':\n",
    "        return False\n",
    "    elif data == '[removed]':\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parent(pid):\n",
    "    try:\n",
    "        sql = \"SELECT comment FROM parent_reply WHERE comment_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_existing_score(pid):\n",
    "    try:\n",
    "        sql = \"SELECT score FROM parent_reply WHERE parent_id = '{}' LIMIT 1\".format(pid)\n",
    "        c.execute(sql)\n",
    "        result = c.fetchone()\n",
    "        if result != None:\n",
    "            return result[0]\n",
    "        else: return False\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Read: 100000, Paired Rows: 6105, Time: 2018-01-17 13:36:05.675456\n",
      "Total Rows Read: 200000, Paired Rows: 13837, Time: 2018-01-17 13:36:33.782940\n",
      "Total Rows Read: 300000, Paired Rows: 22238, Time: 2018-01-17 13:37:02.740443\n",
      "Total Rows Read: 400000, Paired Rows: 30681, Time: 2018-01-17 13:37:30.840582\n",
      "Total Rows Read: 500000, Paired Rows: 38915, Time: 2018-01-17 13:37:59.507318\n",
      "Total Rows Read: 600000, Paired Rows: 46399, Time: 2018-01-17 13:38:27.620751\n",
      "Total Rows Read: 700000, Paired Rows: 54145, Time: 2018-01-17 13:38:55.969912\n",
      "Total Rows Read: 800000, Paired Rows: 62623, Time: 2018-01-17 13:39:25.344709\n",
      "Total Rows Read: 900000, Paired Rows: 71345, Time: 2018-01-17 13:39:53.769487\n",
      "Total Rows Read: 1000000, Paired Rows: 79923, Time: 2018-01-17 13:40:21.310884\n",
      "Cleanin up!\n",
      "Total Rows Read: 1100000, Paired Rows: 87076, Time: 2018-01-17 13:40:54.986833\n",
      "Total Rows Read: 1200000, Paired Rows: 95201, Time: 2018-01-17 13:41:22.871705\n",
      "Total Rows Read: 1300000, Paired Rows: 103703, Time: 2018-01-17 13:41:50.875743\n",
      "Total Rows Read: 1400000, Paired Rows: 112124, Time: 2018-01-17 13:42:19.022210\n",
      "Total Rows Read: 1500000, Paired Rows: 120254, Time: 2018-01-17 13:42:47.189951\n",
      "Total Rows Read: 1600000, Paired Rows: 128810, Time: 2018-01-17 13:43:14.764411\n",
      "Total Rows Read: 1700000, Paired Rows: 137109, Time: 2018-01-17 13:43:42.666118\n",
      "Total Rows Read: 1800000, Paired Rows: 145262, Time: 2018-01-17 13:44:10.441652\n",
      "Total Rows Read: 1900000, Paired Rows: 152936, Time: 2018-01-17 13:44:37.617956\n",
      "Total Rows Read: 2000000, Paired Rows: 162047, Time: 2018-01-17 13:45:06.584439\n",
      "Cleanin up!\n",
      "Total Rows Read: 2100000, Paired Rows: 168603, Time: 2018-01-17 13:45:41.788644\n",
      "Total Rows Read: 2200000, Paired Rows: 176125, Time: 2018-01-17 13:46:11.358290\n",
      "Total Rows Read: 2300000, Paired Rows: 184363, Time: 2018-01-17 13:46:40.679701\n",
      "Total Rows Read: 2400000, Paired Rows: 193235, Time: 2018-01-17 13:47:09.707928\n",
      "Total Rows Read: 2500000, Paired Rows: 202631, Time: 2018-01-17 13:47:38.280044\n",
      "Total Rows Read: 2600000, Paired Rows: 212255, Time: 2018-01-17 13:48:07.083743\n",
      "Total Rows Read: 2700000, Paired Rows: 221844, Time: 2018-01-17 13:48:34.544156\n",
      "Total Rows Read: 2800000, Paired Rows: 231256, Time: 2018-01-17 13:49:02.599256\n",
      "Total Rows Read: 2900000, Paired Rows: 240473, Time: 2018-01-17 13:49:30.824556\n",
      "Total Rows Read: 3000000, Paired Rows: 249668, Time: 2018-01-17 13:49:58.122378\n",
      "Cleanin up!\n",
      "Total Rows Read: 3100000, Paired Rows: 257142, Time: 2018-01-17 13:50:34.755014\n",
      "Total Rows Read: 3200000, Paired Rows: 265674, Time: 2018-01-17 13:51:03.398109\n",
      "Total Rows Read: 3300000, Paired Rows: 274366, Time: 2018-01-17 13:51:32.069509\n",
      "Total Rows Read: 3400000, Paired Rows: 282893, Time: 2018-01-17 13:52:00.201995\n",
      "Total Rows Read: 3500000, Paired Rows: 291749, Time: 2018-01-17 13:52:28.672591\n",
      "Total Rows Read: 3600000, Paired Rows: 300413, Time: 2018-01-17 13:52:56.615547\n",
      "Total Rows Read: 3700000, Paired Rows: 308968, Time: 2018-01-17 13:53:24.589504\n",
      "Total Rows Read: 3800000, Paired Rows: 317113, Time: 2018-01-17 13:53:53.398597\n",
      "Total Rows Read: 3900000, Paired Rows: 325371, Time: 2018-01-17 13:54:22.280422\n",
      "Total Rows Read: 4000000, Paired Rows: 334058, Time: 2018-01-17 13:54:50.951695\n",
      "Cleanin up!\n",
      "Total Rows Read: 4100000, Paired Rows: 341535, Time: 2018-01-17 13:55:27.928478\n",
      "Total Rows Read: 4200000, Paired Rows: 350299, Time: 2018-01-17 13:55:56.163447\n",
      "Total Rows Read: 4300000, Paired Rows: 359167, Time: 2018-01-17 13:56:24.091937\n",
      "Total Rows Read: 4400000, Paired Rows: 368131, Time: 2018-01-17 13:56:51.892294\n",
      "Total Rows Read: 4500000, Paired Rows: 376861, Time: 2018-01-17 13:57:19.752224\n",
      "Total Rows Read: 4600000, Paired Rows: 385545, Time: 2018-01-17 13:57:47.841868\n",
      "Total Rows Read: 4700000, Paired Rows: 393810, Time: 2018-01-17 13:58:17.498385\n",
      "Total Rows Read: 4800000, Paired Rows: 402815, Time: 2018-01-17 13:58:46.657980\n",
      "Total Rows Read: 4900000, Paired Rows: 411503, Time: 2018-01-17 13:59:16.544903\n",
      "Total Rows Read: 5000000, Paired Rows: 419702, Time: 2018-01-17 13:59:45.331660\n",
      "Cleanin up!\n",
      "Total Rows Read: 5100000, Paired Rows: 427026, Time: 2018-01-17 14:00:23.181682\n",
      "Total Rows Read: 5200000, Paired Rows: 435443, Time: 2018-01-17 14:00:51.879859\n",
      "Total Rows Read: 5300000, Paired Rows: 443986, Time: 2018-01-17 14:01:20.668190\n",
      "Total Rows Read: 5400000, Paired Rows: 452099, Time: 2018-01-17 14:01:49.224769\n",
      "Total Rows Read: 5500000, Paired Rows: 460449, Time: 2018-01-17 14:02:18.121356\n",
      "Total Rows Read: 5600000, Paired Rows: 469010, Time: 2018-01-17 14:02:47.521486\n",
      "Total Rows Read: 5700000, Paired Rows: 477724, Time: 2018-01-17 14:03:16.270107\n",
      "Total Rows Read: 5800000, Paired Rows: 486525, Time: 2018-01-17 14:03:44.007602\n",
      "Total Rows Read: 5900000, Paired Rows: 495291, Time: 2018-01-17 14:04:12.020590\n",
      "Total Rows Read: 6000000, Paired Rows: 504238, Time: 2018-01-17 14:04:40.172090\n",
      "Cleanin up!\n",
      "Total Rows Read: 6100000, Paired Rows: 511525, Time: 2018-01-17 14:05:20.725073\n",
      "Total Rows Read: 6200000, Paired Rows: 519462, Time: 2018-01-17 14:05:49.200809\n",
      "Total Rows Read: 6300000, Paired Rows: 527579, Time: 2018-01-17 14:06:17.168462\n",
      "Total Rows Read: 6400000, Paired Rows: 535360, Time: 2018-01-17 14:06:45.799526\n",
      "Total Rows Read: 6500000, Paired Rows: 544399, Time: 2018-01-17 14:07:13.680844\n",
      "Total Rows Read: 6600000, Paired Rows: 553587, Time: 2018-01-17 14:07:41.289217\n",
      "Total Rows Read: 6700000, Paired Rows: 562852, Time: 2018-01-17 14:08:11.288153\n",
      "Total Rows Read: 6800000, Paired Rows: 572004, Time: 2018-01-17 14:08:39.682403\n",
      "Total Rows Read: 6900000, Paired Rows: 581107, Time: 2018-01-17 14:09:08.882014\n",
      "Total Rows Read: 7000000, Paired Rows: 589631, Time: 2018-01-17 14:09:37.334894\n",
      "Cleanin up!\n",
      "Total Rows Read: 7100000, Paired Rows: 596443, Time: 2018-01-17 14:10:18.004157\n",
      "Total Rows Read: 7200000, Paired Rows: 604660, Time: 2018-01-17 14:10:47.253168\n",
      "Total Rows Read: 7300000, Paired Rows: 613536, Time: 2018-01-17 14:11:16.712926\n",
      "Total Rows Read: 7400000, Paired Rows: 622971, Time: 2018-01-17 14:11:46.414532\n",
      "Total Rows Read: 7500000, Paired Rows: 632624, Time: 2018-01-17 14:12:15.363039\n",
      "Total Rows Read: 7600000, Paired Rows: 642616, Time: 2018-01-17 14:12:44.612272\n",
      "Total Rows Read: 7700000, Paired Rows: 651925, Time: 2018-01-17 14:13:13.053018\n",
      "Total Rows Read: 7800000, Paired Rows: 660700, Time: 2018-01-17 14:13:42.829367\n",
      "Total Rows Read: 7900000, Paired Rows: 670287, Time: 2018-01-17 14:14:11.193228\n",
      "Total Rows Read: 8000000, Paired Rows: 679742, Time: 2018-01-17 14:14:38.919804\n",
      "Cleanin up!\n",
      "Total Rows Read: 8100000, Paired Rows: 687076, Time: 2018-01-17 14:15:20.816916\n",
      "Total Rows Read: 8200000, Paired Rows: 695562, Time: 2018-01-17 14:15:49.779691\n",
      "Total Rows Read: 8300000, Paired Rows: 703727, Time: 2018-01-17 14:16:17.416848\n",
      "Total Rows Read: 8400000, Paired Rows: 711960, Time: 2018-01-17 14:16:45.718014\n",
      "Total Rows Read: 8500000, Paired Rows: 720476, Time: 2018-01-17 14:17:14.486197\n",
      "Total Rows Read: 8600000, Paired Rows: 729350, Time: 2018-01-17 14:17:43.971741\n",
      "Total Rows Read: 8700000, Paired Rows: 737873, Time: 2018-01-17 14:18:13.267257\n",
      "Total Rows Read: 8800000, Paired Rows: 746145, Time: 2018-01-17 14:18:42.687557\n",
      "Total Rows Read: 8900000, Paired Rows: 754551, Time: 2018-01-17 14:19:12.840226\n",
      "Total Rows Read: 9000000, Paired Rows: 763530, Time: 2018-01-17 14:19:42.196386\n",
      "Cleanin up!\n",
      "Total Rows Read: 9100000, Paired Rows: 771079, Time: 2018-01-17 14:20:26.491941\n",
      "Total Rows Read: 9200000, Paired Rows: 780412, Time: 2018-01-17 14:20:54.975909\n",
      "Total Rows Read: 9300000, Paired Rows: 790054, Time: 2018-01-17 14:21:24.331901\n",
      "Total Rows Read: 9400000, Paired Rows: 799637, Time: 2018-01-17 14:21:53.237115\n",
      "Total Rows Read: 9500000, Paired Rows: 809377, Time: 2018-01-17 14:22:21.909574\n",
      "Total Rows Read: 9600000, Paired Rows: 819280, Time: 2018-01-17 14:22:51.124194\n",
      "Total Rows Read: 9700000, Paired Rows: 829020, Time: 2018-01-17 14:23:20.494320\n",
      "Total Rows Read: 9800000, Paired Rows: 838623, Time: 2018-01-17 14:23:50.177337\n",
      "Total Rows Read: 9900000, Paired Rows: 848037, Time: 2018-01-17 14:24:19.456327\n",
      "Total Rows Read: 10000000, Paired Rows: 857319, Time: 2018-01-17 14:24:48.158729\n",
      "Cleanin up!\n",
      "Total Rows Read: 10100000, Paired Rows: 864497, Time: 2018-01-17 14:25:32.584280\n",
      "Total Rows Read: 10200000, Paired Rows: 873041, Time: 2018-01-17 14:26:01.266935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Read: 10300000, Paired Rows: 881670, Time: 2018-01-17 14:26:29.723956\n",
      "Total Rows Read: 10400000, Paired Rows: 890341, Time: 2018-01-17 14:26:59.554022\n",
      "Total Rows Read: 10500000, Paired Rows: 898732, Time: 2018-01-17 14:27:29.542047\n",
      "Total Rows Read: 10600000, Paired Rows: 906661, Time: 2018-01-17 14:27:59.650136\n",
      "Total Rows Read: 10700000, Paired Rows: 914864, Time: 2018-01-17 14:28:27.261199\n",
      "Total Rows Read: 10800000, Paired Rows: 923680, Time: 2018-01-17 14:28:55.667190\n",
      "Total Rows Read: 10900000, Paired Rows: 932871, Time: 2018-01-17 14:29:24.024389\n",
      "Total Rows Read: 11000000, Paired Rows: 942661, Time: 2018-01-17 14:29:52.581975\n",
      "Cleanin up!\n",
      "Total Rows Read: 11100000, Paired Rows: 950508, Time: 2018-01-17 14:30:39.237295\n",
      "Total Rows Read: 11200000, Paired Rows: 959841, Time: 2018-01-17 14:31:07.767251\n",
      "Total Rows Read: 11300000, Paired Rows: 969363, Time: 2018-01-17 14:31:36.881649\n",
      "Total Rows Read: 11400000, Paired Rows: 979253, Time: 2018-01-17 14:32:05.989702\n",
      "Total Rows Read: 11500000, Paired Rows: 989042, Time: 2018-01-17 14:32:35.763809\n",
      "Total Rows Read: 11600000, Paired Rows: 998593, Time: 2018-01-17 14:33:05.340636\n",
      "Total Rows Read: 11700000, Paired Rows: 1008028, Time: 2018-01-17 14:33:34.314137\n",
      "Total Rows Read: 11800000, Paired Rows: 1017114, Time: 2018-01-17 14:34:02.645156\n",
      "Total Rows Read: 11900000, Paired Rows: 1026148, Time: 2018-01-17 14:34:31.141154\n",
      "Total Rows Read: 12000000, Paired Rows: 1035372, Time: 2018-01-17 14:34:58.580201\n",
      "Cleanin up!\n",
      "Total Rows Read: 12100000, Paired Rows: 1042624, Time: 2018-01-17 14:35:45.056899\n",
      "Total Rows Read: 12200000, Paired Rows: 1050956, Time: 2018-01-17 14:36:13.080861\n",
      "Total Rows Read: 12300000, Paired Rows: 1059625, Time: 2018-01-17 14:36:41.859097\n",
      "Total Rows Read: 12400000, Paired Rows: 1067863, Time: 2018-01-17 14:37:09.950204\n",
      "Total Rows Read: 12500000, Paired Rows: 1075988, Time: 2018-01-17 14:37:39.000607\n",
      "Total Rows Read: 12600000, Paired Rows: 1084743, Time: 2018-01-17 14:38:07.847864\n",
      "Total Rows Read: 12700000, Paired Rows: 1094118, Time: 2018-01-17 14:38:36.905358\n",
      "Total Rows Read: 12800000, Paired Rows: 1103845, Time: 2018-01-17 14:39:05.409848\n",
      "Total Rows Read: 12900000, Paired Rows: 1113611, Time: 2018-01-17 14:39:33.955006\n",
      "Total Rows Read: 13000000, Paired Rows: 1123692, Time: 2018-01-17 14:40:02.238595\n",
      "Cleanin up!\n",
      "Total Rows Read: 13100000, Paired Rows: 1131737, Time: 2018-01-17 14:40:50.486146\n",
      "Total Rows Read: 13200000, Paired Rows: 1141231, Time: 2018-01-17 14:41:19.889680\n",
      "Total Rows Read: 13300000, Paired Rows: 1151214, Time: 2018-01-17 14:41:48.579598\n",
      "Total Rows Read: 13400000, Paired Rows: 1160212, Time: 2018-01-17 14:42:17.609182\n",
      "Total Rows Read: 13500000, Paired Rows: 1169506, Time: 2018-01-17 14:42:47.625984\n",
      "Total Rows Read: 13600000, Paired Rows: 1178734, Time: 2018-01-17 14:43:17.318932\n",
      "Total Rows Read: 13700000, Paired Rows: 1187625, Time: 2018-01-17 14:43:46.278380\n",
      "Total Rows Read: 13800000, Paired Rows: 1196810, Time: 2018-01-17 14:44:15.221091\n",
      "Total Rows Read: 13900000, Paired Rows: 1205616, Time: 2018-01-17 14:44:44.093010\n",
      "Total Rows Read: 14000000, Paired Rows: 1214829, Time: 2018-01-17 14:45:12.562864\n",
      "Cleanin up!\n",
      "Total Rows Read: 14100000, Paired Rows: 1222009, Time: 2018-01-17 14:46:03.030587\n",
      "Total Rows Read: 14200000, Paired Rows: 1229772, Time: 2018-01-17 14:46:32.249366\n",
      "Total Rows Read: 14300000, Paired Rows: 1237369, Time: 2018-01-17 14:47:01.973922\n",
      "Total Rows Read: 14400000, Paired Rows: 1245738, Time: 2018-01-17 14:47:31.271806\n",
      "Total Rows Read: 14500000, Paired Rows: 1254575, Time: 2018-01-17 14:48:00.946798\n",
      "Total Rows Read: 14600000, Paired Rows: 1263927, Time: 2018-01-17 14:48:30.484550\n",
      "Total Rows Read: 14700000, Paired Rows: 1273456, Time: 2018-01-17 14:48:58.954245\n",
      "Total Rows Read: 14800000, Paired Rows: 1283283, Time: 2018-01-17 14:49:28.463154\n",
      "Total Rows Read: 14900000, Paired Rows: 1293044, Time: 2018-01-17 14:49:57.351372\n",
      "Total Rows Read: 15000000, Paired Rows: 1303122, Time: 2018-01-17 14:50:27.186732\n",
      "Cleanin up!\n",
      "Total Rows Read: 15100000, Paired Rows: 1311136, Time: 2018-01-17 14:51:17.650024\n",
      "Total Rows Read: 15200000, Paired Rows: 1320576, Time: 2018-01-17 14:51:47.794943\n",
      "Total Rows Read: 15300000, Paired Rows: 1329903, Time: 2018-01-17 14:52:17.009452\n",
      "Total Rows Read: 15400000, Paired Rows: 1338966, Time: 2018-01-17 14:52:47.056877\n",
      "Total Rows Read: 15500000, Paired Rows: 1347807, Time: 2018-01-17 14:53:15.011928\n",
      "Total Rows Read: 15600000, Paired Rows: 1356464, Time: 2018-01-17 14:53:44.520148\n",
      "Total Rows Read: 15700000, Paired Rows: 1364945, Time: 2018-01-17 14:54:15.034781\n",
      "Total Rows Read: 15800000, Paired Rows: 1373575, Time: 2018-01-17 14:54:44.174206\n",
      "Total Rows Read: 15900000, Paired Rows: 1382068, Time: 2018-01-17 14:55:12.534153\n",
      "Total Rows Read: 16000000, Paired Rows: 1390131, Time: 2018-01-17 14:55:40.541258\n",
      "Cleanin up!\n",
      "Total Rows Read: 16100000, Paired Rows: 1396449, Time: 2018-01-17 14:56:33.120317\n",
      "Total Rows Read: 16200000, Paired Rows: 1404331, Time: 2018-01-17 14:57:02.306738\n",
      "Total Rows Read: 16300000, Paired Rows: 1412947, Time: 2018-01-17 14:57:31.005032\n",
      "Total Rows Read: 16400000, Paired Rows: 1422025, Time: 2018-01-17 14:58:00.459060\n",
      "Total Rows Read: 16500000, Paired Rows: 1431049, Time: 2018-01-17 14:58:29.019929\n",
      "Total Rows Read: 16600000, Paired Rows: 1440409, Time: 2018-01-17 14:58:58.440669\n",
      "Total Rows Read: 16700000, Paired Rows: 1449762, Time: 2018-01-17 14:59:27.160051\n",
      "Total Rows Read: 16800000, Paired Rows: 1458025, Time: 2018-01-17 14:59:54.135212\n",
      "Total Rows Read: 16900000, Paired Rows: 1466293, Time: 2018-01-17 15:00:22.461025\n",
      "Total Rows Read: 17000000, Paired Rows: 1474294, Time: 2018-01-17 15:00:50.192857\n",
      "Cleanin up!\n",
      "Total Rows Read: 17100000, Paired Rows: 1481043, Time: 2018-01-17 15:01:43.329530\n",
      "Total Rows Read: 17200000, Paired Rows: 1489089, Time: 2018-01-17 15:02:11.160637\n",
      "Total Rows Read: 17300000, Paired Rows: 1497309, Time: 2018-01-17 15:02:39.296773\n",
      "Total Rows Read: 17400000, Paired Rows: 1506413, Time: 2018-01-17 15:03:08.713882\n",
      "Total Rows Read: 17500000, Paired Rows: 1515271, Time: 2018-01-17 15:03:38.602121\n",
      "Total Rows Read: 17600000, Paired Rows: 1523634, Time: 2018-01-17 15:04:09.187339\n",
      "Total Rows Read: 17700000, Paired Rows: 1531635, Time: 2018-01-17 15:04:37.304741\n",
      "Total Rows Read: 17800000, Paired Rows: 1539899, Time: 2018-01-17 15:05:07.022496\n",
      "Total Rows Read: 17900000, Paired Rows: 1548743, Time: 2018-01-17 15:05:36.066738\n",
      "Total Rows Read: 18000000, Paired Rows: 1557593, Time: 2018-01-17 15:06:04.127730\n",
      "Cleanin up!\n",
      "Total Rows Read: 18100000, Paired Rows: 1564483, Time: 2018-01-17 15:06:58.671667\n",
      "Total Rows Read: 18200000, Paired Rows: 1572820, Time: 2018-01-17 15:07:27.763674\n",
      "Total Rows Read: 18300000, Paired Rows: 1580609, Time: 2018-01-17 15:07:56.304777\n",
      "Total Rows Read: 18400000, Paired Rows: 1588981, Time: 2018-01-17 15:08:25.075185\n",
      "Total Rows Read: 18500000, Paired Rows: 1597989, Time: 2018-01-17 15:08:54.974984\n",
      "Total Rows Read: 18600000, Paired Rows: 1606682, Time: 2018-01-17 15:09:24.275661\n",
      "Total Rows Read: 18700000, Paired Rows: 1615126, Time: 2018-01-17 15:09:52.619096\n",
      "Total Rows Read: 18800000, Paired Rows: 1624384, Time: 2018-01-17 15:10:20.587513\n",
      "Total Rows Read: 18900000, Paired Rows: 1633565, Time: 2018-01-17 15:10:49.367177\n",
      "Total Rows Read: 19000000, Paired Rows: 1642739, Time: 2018-01-17 15:11:17.753508\n",
      "Cleanin up!\n",
      "Total Rows Read: 19100000, Paired Rows: 1649866, Time: 2018-01-17 15:12:14.312859\n",
      "Total Rows Read: 19200000, Paired Rows: 1658185, Time: 2018-01-17 15:12:44.985966\n",
      "Total Rows Read: 19300000, Paired Rows: 1666425, Time: 2018-01-17 15:13:14.972469\n",
      "Total Rows Read: 19400000, Paired Rows: 1675169, Time: 2018-01-17 15:13:45.172722\n",
      "Total Rows Read: 19500000, Paired Rows: 1684243, Time: 2018-01-17 15:14:15.186958\n",
      "Total Rows Read: 19600000, Paired Rows: 1693851, Time: 2018-01-17 15:14:44.244645\n",
      "Total Rows Read: 19700000, Paired Rows: 1703670, Time: 2018-01-17 15:15:14.278808\n",
      "Total Rows Read: 19800000, Paired Rows: 1713423, Time: 2018-01-17 15:15:44.149940\n",
      "Total Rows Read: 19900000, Paired Rows: 1723084, Time: 2018-01-17 15:16:13.421678\n",
      "Total Rows Read: 20000000, Paired Rows: 1732962, Time: 2018-01-17 15:16:42.429739\n",
      "Cleanin up!\n",
      "Total Rows Read: 20100000, Paired Rows: 1741031, Time: 2018-01-17 15:17:43.311257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Read: 20200000, Paired Rows: 1750493, Time: 2018-01-17 15:18:13.642261\n",
      "Total Rows Read: 20300000, Paired Rows: 1759576, Time: 2018-01-17 15:18:45.007510\n",
      "Total Rows Read: 20400000, Paired Rows: 1768972, Time: 2018-01-17 15:19:15.485518\n",
      "Total Rows Read: 20500000, Paired Rows: 1777355, Time: 2018-01-17 15:19:45.948304\n",
      "Total Rows Read: 20600000, Paired Rows: 1785345, Time: 2018-01-17 15:20:15.364474\n",
      "Total Rows Read: 20700000, Paired Rows: 1793385, Time: 2018-01-17 15:20:44.799044\n",
      "Total Rows Read: 20800000, Paired Rows: 1801389, Time: 2018-01-17 15:21:12.678330\n",
      "Total Rows Read: 20900000, Paired Rows: 1809957, Time: 2018-01-17 15:21:41.390776\n",
      "Total Rows Read: 21000000, Paired Rows: 1818270, Time: 2018-01-17 15:22:11.130434\n",
      "Cleanin up!\n",
      "Total Rows Read: 21100000, Paired Rows: 1824409, Time: 2018-01-17 15:23:11.010786\n",
      "Total Rows Read: 21200000, Paired Rows: 1831933, Time: 2018-01-17 15:23:41.980638\n",
      "Total Rows Read: 21300000, Paired Rows: 1840401, Time: 2018-01-17 15:24:11.700413\n",
      "Total Rows Read: 21400000, Paired Rows: 1849631, Time: 2018-01-17 15:24:42.801655\n",
      "Total Rows Read: 21500000, Paired Rows: 1859084, Time: 2018-01-17 15:25:11.959916\n",
      "Total Rows Read: 21600000, Paired Rows: 1868617, Time: 2018-01-17 15:25:43.523512\n",
      "Total Rows Read: 21700000, Paired Rows: 1878115, Time: 2018-01-17 15:26:13.888253\n",
      "Total Rows Read: 21800000, Paired Rows: 1887530, Time: 2018-01-17 15:26:44.376588\n",
      "Total Rows Read: 21900000, Paired Rows: 1896642, Time: 2018-01-17 15:27:13.816598\n",
      "Total Rows Read: 22000000, Paired Rows: 1905526, Time: 2018-01-17 15:27:43.277010\n",
      "Cleanin up!\n",
      "Total Rows Read: 22100000, Paired Rows: 1912283, Time: 2018-01-17 15:28:47.038544\n",
      "Total Rows Read: 22200000, Paired Rows: 1920728, Time: 2018-01-17 15:29:17.502351\n",
      "Total Rows Read: 22300000, Paired Rows: 1929428, Time: 2018-01-17 15:29:48.763997\n",
      "Total Rows Read: 22400000, Paired Rows: 1938246, Time: 2018-01-17 15:30:20.880309\n",
      "Total Rows Read: 22500000, Paired Rows: 1947097, Time: 2018-01-17 15:30:50.662138\n",
      "Total Rows Read: 22600000, Paired Rows: 1956227, Time: 2018-01-17 15:31:21.764168\n",
      "Total Rows Read: 22700000, Paired Rows: 1965227, Time: 2018-01-17 15:31:51.677239\n",
      "Total Rows Read: 22800000, Paired Rows: 1973922, Time: 2018-01-17 15:32:21.914717\n",
      "Total Rows Read: 22900000, Paired Rows: 1981827, Time: 2018-01-17 15:32:53.781247\n",
      "Total Rows Read: 23000000, Paired Rows: 1989994, Time: 2018-01-17 15:33:23.446139\n",
      "Cleanin up!\n",
      "Total Rows Read: 23100000, Paired Rows: 1996955, Time: 2018-01-17 15:34:23.691985\n",
      "Total Rows Read: 23200000, Paired Rows: 2005673, Time: 2018-01-17 15:34:53.838793\n",
      "Total Rows Read: 23300000, Paired Rows: 2015138, Time: 2018-01-17 15:35:25.533912\n",
      "Total Rows Read: 23400000, Paired Rows: 2024617, Time: 2018-01-17 15:35:55.899719\n",
      "Total Rows Read: 23500000, Paired Rows: 2033859, Time: 2018-01-17 15:36:26.608971\n",
      "Total Rows Read: 23600000, Paired Rows: 2043142, Time: 2018-01-17 15:36:55.430825\n",
      "Total Rows Read: 23700000, Paired Rows: 2052885, Time: 2018-01-17 15:37:23.668795\n",
      "Total Rows Read: 23800000, Paired Rows: 2062472, Time: 2018-01-17 15:37:52.581614\n",
      "Total Rows Read: 23900000, Paired Rows: 2072039, Time: 2018-01-17 15:38:20.141675\n",
      "Total Rows Read: 24000000, Paired Rows: 2081811, Time: 2018-01-17 15:38:49.538933\n",
      "Cleanin up!\n",
      "Total Rows Read: 24100000, Paired Rows: 2089166, Time: 2018-01-17 15:39:52.719245\n",
      "Total Rows Read: 24200000, Paired Rows: 2097799, Time: 2018-01-17 15:40:21.656744\n",
      "Total Rows Read: 24300000, Paired Rows: 2106612, Time: 2018-01-17 15:40:51.937878\n",
      "Total Rows Read: 24400000, Paired Rows: 2115322, Time: 2018-01-17 15:41:21.030971\n",
      "Total Rows Read: 24500000, Paired Rows: 2123980, Time: 2018-01-17 15:41:50.805664\n",
      "Total Rows Read: 24600000, Paired Rows: 2132715, Time: 2018-01-17 15:42:22.234486\n",
      "Total Rows Read: 24700000, Paired Rows: 2141093, Time: 2018-01-17 15:42:52.433307\n",
      "Total Rows Read: 24800000, Paired Rows: 2149545, Time: 2018-01-17 15:43:22.997222\n",
      "Total Rows Read: 24900000, Paired Rows: 2158500, Time: 2018-01-17 15:43:52.758265\n",
      "Total Rows Read: 25000000, Paired Rows: 2167866, Time: 2018-01-17 15:44:22.079728\n",
      "Cleanin up!\n",
      "Total Rows Read: 25100000, Paired Rows: 2175619, Time: 2018-01-17 15:45:29.188802\n",
      "Total Rows Read: 25200000, Paired Rows: 2184872, Time: 2018-01-17 15:46:00.150595\n",
      "Total Rows Read: 25300000, Paired Rows: 2194234, Time: 2018-01-17 15:46:30.490348\n",
      "Total Rows Read: 25400000, Paired Rows: 2203908, Time: 2018-01-17 15:47:00.322913\n",
      "Total Rows Read: 25500000, Paired Rows: 2213622, Time: 2018-01-17 15:47:30.443561\n",
      "Total Rows Read: 25600000, Paired Rows: 2223272, Time: 2018-01-17 15:48:00.367043\n",
      "Total Rows Read: 25700000, Paired Rows: 2233142, Time: 2018-01-17 15:48:29.254762\n",
      "Total Rows Read: 25800000, Paired Rows: 2242606, Time: 2018-01-17 15:48:58.946197\n",
      "Total Rows Read: 25900000, Paired Rows: 2251890, Time: 2018-01-17 15:49:28.874809\n",
      "Total Rows Read: 26000000, Paired Rows: 2260937, Time: 2018-01-17 15:49:58.132349\n",
      "Cleanin up!\n",
      "Total Rows Read: 26100000, Paired Rows: 2268042, Time: 2018-01-17 15:51:05.332981\n",
      "Total Rows Read: 26200000, Paired Rows: 2276408, Time: 2018-01-17 15:51:35.561167\n",
      "Total Rows Read: 26300000, Paired Rows: 2285314, Time: 2018-01-17 15:52:05.463444\n",
      "Total Rows Read: 26400000, Paired Rows: 2293799, Time: 2018-01-17 15:52:37.013982\n",
      "Total Rows Read: 26500000, Paired Rows: 2302153, Time: 2018-01-17 15:53:06.820359\n",
      "Total Rows Read: 26600000, Paired Rows: 2309943, Time: 2018-01-17 15:53:37.044006\n",
      "Total Rows Read: 26700000, Paired Rows: 2318176, Time: 2018-01-17 15:54:07.044177\n",
      "Total Rows Read: 26800000, Paired Rows: 2327076, Time: 2018-01-17 15:54:37.664203\n",
      "Total Rows Read: 26900000, Paired Rows: 2336400, Time: 2018-01-17 15:55:05.985388\n",
      "Total Rows Read: 27000000, Paired Rows: 2345763, Time: 2018-01-17 15:55:35.265114\n",
      "Cleanin up!\n",
      "Total Rows Read: 27100000, Paired Rows: 2353532, Time: 2018-01-17 15:56:45.110052\n",
      "Total Rows Read: 27200000, Paired Rows: 2362832, Time: 2018-01-17 15:57:15.043987\n",
      "Total Rows Read: 27300000, Paired Rows: 2372467, Time: 2018-01-17 15:57:46.058563\n",
      "Total Rows Read: 27400000, Paired Rows: 2382061, Time: 2018-01-17 15:58:15.531721\n",
      "Total Rows Read: 27500000, Paired Rows: 2391592, Time: 2018-01-17 15:58:43.723464\n",
      "Total Rows Read: 27600000, Paired Rows: 2400905, Time: 2018-01-17 15:59:12.890937\n",
      "Total Rows Read: 27700000, Paired Rows: 2409949, Time: 2018-01-17 15:59:41.908891\n",
      "Total Rows Read: 27800000, Paired Rows: 2418865, Time: 2018-01-17 16:00:10.885785\n",
      "Total Rows Read: 27900000, Paired Rows: 2427323, Time: 2018-01-17 16:00:40.169469\n",
      "Total Rows Read: 28000000, Paired Rows: 2435966, Time: 2018-01-17 16:01:09.394884\n",
      "Cleanin up!\n",
      "Total Rows Read: 28100000, Paired Rows: 2442887, Time: 2018-01-17 16:02:18.493578\n",
      "Total Rows Read: 28200000, Paired Rows: 2450657, Time: 2018-01-17 16:02:48.405889\n",
      "Total Rows Read: 28300000, Paired Rows: 2458362, Time: 2018-01-17 16:03:18.098211\n",
      "Total Rows Read: 28400000, Paired Rows: 2465812, Time: 2018-01-17 16:03:47.899375\n",
      "Total Rows Read: 28500000, Paired Rows: 2474100, Time: 2018-01-17 16:04:17.277462\n",
      "Total Rows Read: 28600000, Paired Rows: 2482486, Time: 2018-01-17 16:04:46.214361\n",
      "Total Rows Read: 28700000, Paired Rows: 2491554, Time: 2018-01-17 16:05:14.863302\n",
      "Total Rows Read: 28800000, Paired Rows: 2500919, Time: 2018-01-17 16:05:43.940034\n",
      "Total Rows Read: 28900000, Paired Rows: 2510378, Time: 2018-01-17 16:06:13.568604\n",
      "Total Rows Read: 29000000, Paired Rows: 2519724, Time: 2018-01-17 16:06:44.770166\n",
      "Cleanin up!\n",
      "Total Rows Read: 29100000, Paired Rows: 2527238, Time: 2018-01-17 16:07:56.799319\n",
      "Total Rows Read: 29200000, Paired Rows: 2536325, Time: 2018-01-17 16:08:27.425933\n",
      "Total Rows Read: 29300000, Paired Rows: 2545428, Time: 2018-01-17 16:08:57.318291\n",
      "Total Rows Read: 29400000, Paired Rows: 2554235, Time: 2018-01-17 16:09:26.501168\n",
      "Total Rows Read: 29500000, Paired Rows: 2563038, Time: 2018-01-17 16:09:54.778324\n",
      "Total Rows Read: 29600000, Paired Rows: 2571933, Time: 2018-01-17 16:10:24.889328\n",
      "Total Rows Read: 29700000, Paired Rows: 2580559, Time: 2018-01-17 16:10:54.956276\n",
      "Total Rows Read: 29800000, Paired Rows: 2588542, Time: 2018-01-17 16:11:25.292410\n",
      "Total Rows Read: 29900000, Paired Rows: 2596602, Time: 2018-01-17 16:11:53.968561\n",
      "Total Rows Read: 30000000, Paired Rows: 2605466, Time: 2018-01-17 16:12:23.733863\n",
      "Cleanin up!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Read: 30100000, Paired Rows: 2612803, Time: 2018-01-17 16:13:33.655022\n",
      "Total Rows Read: 30200000, Paired Rows: 2621430, Time: 2018-01-17 16:14:03.597034\n",
      "Total Rows Read: 30300000, Paired Rows: 2630552, Time: 2018-01-17 16:14:33.493662\n",
      "Total Rows Read: 30400000, Paired Rows: 2638650, Time: 2018-01-17 16:15:02.386530\n",
      "Total Rows Read: 30500000, Paired Rows: 2646280, Time: 2018-01-17 16:15:29.781881\n",
      "Total Rows Read: 30600000, Paired Rows: 2654124, Time: 2018-01-17 16:15:57.810794\n",
      "Total Rows Read: 30700000, Paired Rows: 2660741, Time: 2018-01-17 16:16:26.571750\n",
      "Total Rows Read: 30800000, Paired Rows: 2668925, Time: 2018-01-17 16:16:57.950362\n",
      "Total Rows Read: 30900000, Paired Rows: 2677493, Time: 2018-01-17 16:17:29.312314\n",
      "Total Rows Read: 31000000, Paired Rows: 2685640, Time: 2018-01-17 16:17:58.729805\n",
      "Cleanin up!\n",
      "Total Rows Read: 31100000, Paired Rows: 2693005, Time: 2018-01-17 16:19:11.412929\n",
      "Total Rows Read: 31200000, Paired Rows: 2701603, Time: 2018-01-17 16:19:41.816273\n",
      "Total Rows Read: 31300000, Paired Rows: 2710657, Time: 2018-01-17 16:20:13.565785\n",
      "Total Rows Read: 31400000, Paired Rows: 2719312, Time: 2018-01-17 16:20:46.474034\n",
      "Total Rows Read: 31500000, Paired Rows: 2727187, Time: 2018-01-17 16:21:17.538420\n",
      "Total Rows Read: 31600000, Paired Rows: 2735551, Time: 2018-01-17 16:21:48.478111\n",
      "Total Rows Read: 31700000, Paired Rows: 2744341, Time: 2018-01-17 16:22:18.834674\n",
      "Total Rows Read: 31800000, Paired Rows: 2753487, Time: 2018-01-17 16:22:48.250561\n",
      "Total Rows Read: 31900000, Paired Rows: 2762850, Time: 2018-01-17 16:23:18.053454\n",
      "Total Rows Read: 32000000, Paired Rows: 2772466, Time: 2018-01-17 16:23:47.079524\n",
      "Cleanin up!\n",
      "Total Rows Read: 32100000, Paired Rows: 2779728, Time: 2018-01-17 16:25:00.874101\n",
      "Total Rows Read: 32200000, Paired Rows: 2788353, Time: 2018-01-17 16:25:31.103354\n",
      "Total Rows Read: 32300000, Paired Rows: 2797090, Time: 2018-01-17 16:26:01.338964\n",
      "Total Rows Read: 32400000, Paired Rows: 2805547, Time: 2018-01-17 16:26:33.790908\n",
      "Total Rows Read: 32500000, Paired Rows: 2814062, Time: 2018-01-17 16:27:05.830868\n",
      "Total Rows Read: 32600000, Paired Rows: 2822769, Time: 2018-01-17 16:27:37.204920\n",
      "Total Rows Read: 32700000, Paired Rows: 2831278, Time: 2018-01-17 16:28:07.392945\n",
      "Total Rows Read: 32800000, Paired Rows: 2839438, Time: 2018-01-17 16:28:35.866712\n",
      "Total Rows Read: 32900000, Paired Rows: 2847677, Time: 2018-01-17 16:29:04.212778\n",
      "Total Rows Read: 33000000, Paired Rows: 2856417, Time: 2018-01-17 16:29:32.446988\n",
      "Cleanin up!\n",
      "Total Rows Read: 33100000, Paired Rows: 2863193, Time: 2018-01-17 16:30:46.398354\n",
      "Total Rows Read: 33200000, Paired Rows: 2870878, Time: 2018-01-17 16:31:15.623318\n",
      "Total Rows Read: 33300000, Paired Rows: 2878543, Time: 2018-01-17 16:31:44.472934\n",
      "Total Rows Read: 33400000, Paired Rows: 2886921, Time: 2018-01-17 16:32:14.447084\n",
      "Total Rows Read: 33500000, Paired Rows: 2895763, Time: 2018-01-17 16:32:44.940260\n",
      "Total Rows Read: 33600000, Paired Rows: 2905514, Time: 2018-01-17 16:33:14.534456\n",
      "Total Rows Read: 33700000, Paired Rows: 2915244, Time: 2018-01-17 16:33:44.416897\n",
      "Total Rows Read: 33800000, Paired Rows: 2924817, Time: 2018-01-17 16:34:14.385355\n",
      "Total Rows Read: 33900000, Paired Rows: 2934613, Time: 2018-01-17 16:34:44.523582\n",
      "Total Rows Read: 34000000, Paired Rows: 2944471, Time: 2018-01-17 16:35:14.543517\n",
      "Cleanin up!\n",
      "Total Rows Read: 34100000, Paired Rows: 2952469, Time: 2018-01-17 16:36:31.766662\n",
      "Total Rows Read: 34200000, Paired Rows: 2961512, Time: 2018-01-17 16:37:02.139438\n",
      "Total Rows Read: 34300000, Paired Rows: 2970582, Time: 2018-01-17 16:37:33.001013\n",
      "Total Rows Read: 34400000, Paired Rows: 2979808, Time: 2018-01-17 16:38:03.512082\n",
      "Total Rows Read: 34500000, Paired Rows: 2988897, Time: 2018-01-17 16:38:33.274627\n",
      "Total Rows Read: 34600000, Paired Rows: 2997613, Time: 2018-01-17 16:39:02.651206\n",
      "Total Rows Read: 34700000, Paired Rows: 3006156, Time: 2018-01-17 16:39:31.730999\n",
      "Total Rows Read: 34800000, Paired Rows: 3014999, Time: 2018-01-17 16:40:01.086567\n",
      "Total Rows Read: 34900000, Paired Rows: 3023779, Time: 2018-01-17 16:40:29.084806\n",
      "Total Rows Read: 35000000, Paired Rows: 3032271, Time: 2018-01-17 16:40:59.464626\n",
      "Cleanin up!\n",
      "Total Rows Read: 35100000, Paired Rows: 3038416, Time: 2018-01-17 16:42:17.295081\n",
      "Total Rows Read: 35200000, Paired Rows: 3045537, Time: 2018-01-17 16:42:46.318851\n",
      "Total Rows Read: 35300000, Paired Rows: 3053519, Time: 2018-01-17 16:43:16.400533\n",
      "Total Rows Read: 35400000, Paired Rows: 3062096, Time: 2018-01-17 16:43:46.281015\n",
      "Total Rows Read: 35500000, Paired Rows: 3071170, Time: 2018-01-17 16:44:15.496508\n",
      "Total Rows Read: 35600000, Paired Rows: 3080540, Time: 2018-01-17 16:44:43.950699\n",
      "Total Rows Read: 35700000, Paired Rows: 3089944, Time: 2018-01-17 16:45:13.011789\n",
      "Total Rows Read: 35800000, Paired Rows: 3099603, Time: 2018-01-17 16:45:42.019065\n",
      "Total Rows Read: 35900000, Paired Rows: 3109141, Time: 2018-01-17 16:46:11.881219\n",
      "Total Rows Read: 36000000, Paired Rows: 3118611, Time: 2018-01-17 16:46:40.626725\n",
      "Cleanin up!\n",
      "Total Rows Read: 36100000, Paired Rows: 3126249, Time: 2018-01-17 16:48:00.670807\n",
      "Total Rows Read: 36200000, Paired Rows: 3135462, Time: 2018-01-17 16:48:30.797018\n",
      "Total Rows Read: 36300000, Paired Rows: 3144316, Time: 2018-01-17 16:49:00.107495\n",
      "Total Rows Read: 36400000, Paired Rows: 3153323, Time: 2018-01-17 16:49:29.769544\n",
      "Total Rows Read: 36500000, Paired Rows: 3162194, Time: 2018-01-17 16:49:59.036462\n",
      "Total Rows Read: 36600000, Paired Rows: 3171031, Time: 2018-01-17 16:50:27.767326\n",
      "Total Rows Read: 36700000, Paired Rows: 3179661, Time: 2018-01-17 16:50:57.355539\n",
      "Total Rows Read: 36800000, Paired Rows: 3188511, Time: 2018-01-17 16:51:26.357428\n",
      "Total Rows Read: 36900000, Paired Rows: 3197270, Time: 2018-01-17 16:51:56.034213\n",
      "Total Rows Read: 37000000, Paired Rows: 3205565, Time: 2018-01-17 16:52:25.140061\n",
      "Cleanin up!\n",
      "Total Rows Read: 37100000, Paired Rows: 3212142, Time: 2018-01-17 16:53:46.023897\n",
      "Total Rows Read: 37200000, Paired Rows: 3220380, Time: 2018-01-17 16:54:15.933089\n",
      "Total Rows Read: 37300000, Paired Rows: 3229626, Time: 2018-01-17 16:54:44.870592\n",
      "Total Rows Read: 37400000, Paired Rows: 3239098, Time: 2018-01-17 16:55:14.209973\n",
      "Total Rows Read: 37500000, Paired Rows: 3248817, Time: 2018-01-17 16:55:43.854844\n",
      "Total Rows Read: 37600000, Paired Rows: 3258293, Time: 2018-01-17 16:56:14.336237\n",
      "Total Rows Read: 37700000, Paired Rows: 3268108, Time: 2018-01-17 16:56:44.275479\n",
      "Total Rows Read: 37800000, Paired Rows: 3277889, Time: 2018-01-17 16:57:13.399587\n",
      "Total Rows Read: 37900000, Paired Rows: 3287705, Time: 2018-01-17 16:57:42.785940\n",
      "Total Rows Read: 38000000, Paired Rows: 3297412, Time: 2018-01-17 16:58:11.401517\n",
      "Cleanin up!\n",
      "Total Rows Read: 38100000, Paired Rows: 3305161, Time: 2018-01-17 16:59:35.447647\n",
      "Total Rows Read: 38200000, Paired Rows: 3314009, Time: 2018-01-17 17:00:06.179545\n",
      "Total Rows Read: 38300000, Paired Rows: 3322787, Time: 2018-01-17 17:00:36.045564\n",
      "Total Rows Read: 38400000, Paired Rows: 3331693, Time: 2018-01-17 17:01:05.671909\n",
      "Total Rows Read: 38500000, Paired Rows: 3340527, Time: 2018-01-17 17:01:35.060566\n",
      "Total Rows Read: 38600000, Paired Rows: 3349279, Time: 2018-01-17 17:02:04.632679\n",
      "Total Rows Read: 38700000, Paired Rows: 3357739, Time: 2018-01-17 17:02:34.462838\n",
      "Total Rows Read: 38800000, Paired Rows: 3366012, Time: 2018-01-17 17:03:04.265008\n",
      "Total Rows Read: 38900000, Paired Rows: 3373735, Time: 2018-01-17 17:03:33.637632\n",
      "Total Rows Read: 39000000, Paired Rows: 3382080, Time: 2018-01-17 17:04:02.711420\n",
      "Cleanin up!\n",
      "Total Rows Read: 39100000, Paired Rows: 3389284, Time: 2018-01-17 17:05:26.568394\n",
      "Total Rows Read: 39200000, Paired Rows: 3397988, Time: 2018-01-17 17:05:56.355663\n",
      "Total Rows Read: 39300000, Paired Rows: 3407390, Time: 2018-01-17 17:06:26.164967\n",
      "Total Rows Read: 39400000, Paired Rows: 3417003, Time: 2018-01-17 17:06:56.984672\n",
      "Total Rows Read: 39500000, Paired Rows: 3426401, Time: 2018-01-17 17:07:27.851400\n",
      "Total Rows Read: 39600000, Paired Rows: 3436170, Time: 2018-01-17 17:07:58.448205\n",
      "Total Rows Read: 39700000, Paired Rows: 3445467, Time: 2018-01-17 17:08:29.386713\n",
      "Total Rows Read: 39800000, Paired Rows: 3454993, Time: 2018-01-17 17:08:59.209355\n",
      "Total Rows Read: 39900000, Paired Rows: 3464074, Time: 2018-01-17 17:09:28.966499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Read: 40000000, Paired Rows: 3473242, Time: 2018-01-17 17:09:59.109872\n",
      "Cleanin up!\n",
      "Total Rows Read: 40100000, Paired Rows: 3480214, Time: 2018-01-17 17:11:23.158899\n",
      "Total Rows Read: 40200000, Paired Rows: 3488289, Time: 2018-01-17 17:11:52.515765\n",
      "Total Rows Read: 40300000, Paired Rows: 3496714, Time: 2018-01-17 17:12:22.030328\n",
      "Total Rows Read: 40400000, Paired Rows: 3504909, Time: 2018-01-17 17:12:50.887157\n",
      "Total Rows Read: 40500000, Paired Rows: 3513333, Time: 2018-01-17 17:13:20.365093\n",
      "Total Rows Read: 40600000, Paired Rows: 3521484, Time: 2018-01-17 17:13:50.064480\n",
      "Total Rows Read: 40700000, Paired Rows: 3529414, Time: 2018-01-17 17:14:19.779473\n",
      "Total Rows Read: 40800000, Paired Rows: 3537737, Time: 2018-01-17 17:14:48.547522\n",
      "Total Rows Read: 40900000, Paired Rows: 3546431, Time: 2018-01-17 17:15:17.729163\n",
      "Total Rows Read: 41000000, Paired Rows: 3555314, Time: 2018-01-17 17:15:45.449078\n",
      "Cleanin up!\n",
      "Total Rows Read: 41100000, Paired Rows: 3562773, Time: 2018-01-17 17:17:10.105056\n",
      "Total Rows Read: 41200000, Paired Rows: 3571770, Time: 2018-01-17 17:17:39.320696\n",
      "Total Rows Read: 41300000, Paired Rows: 3580844, Time: 2018-01-17 17:18:07.755695\n",
      "Total Rows Read: 41400000, Paired Rows: 3589910, Time: 2018-01-17 17:18:35.745588\n",
      "Total Rows Read: 41500000, Paired Rows: 3598773, Time: 2018-01-17 17:19:03.515643\n",
      "Total Rows Read: 41600000, Paired Rows: 3607268, Time: 2018-01-17 17:19:34.641256\n",
      "Total Rows Read: 41700000, Paired Rows: 3615866, Time: 2018-01-17 17:20:05.889010\n",
      "Total Rows Read: 41800000, Paired Rows: 3624548, Time: 2018-01-17 17:20:35.017721\n",
      "Total Rows Read: 41900000, Paired Rows: 3633094, Time: 2018-01-17 17:21:03.281001\n",
      "Total Rows Read: 42000000, Paired Rows: 3641792, Time: 2018-01-17 17:21:31.790706\n",
      "Cleanin up!\n",
      "Total Rows Read: 42100000, Paired Rows: 3648743, Time: 2018-01-17 17:22:58.251061\n",
      "Total Rows Read: 42200000, Paired Rows: 3656140, Time: 2018-01-17 17:23:28.579904\n",
      "Total Rows Read: 42300000, Paired Rows: 3664083, Time: 2018-01-17 17:23:57.651437\n",
      "Total Rows Read: 42400000, Paired Rows: 3672907, Time: 2018-01-17 17:24:26.614884\n",
      "Total Rows Read: 42500000, Paired Rows: 3681947, Time: 2018-01-17 17:24:55.179141\n",
      "Total Rows Read: 42600000, Paired Rows: 3691305, Time: 2018-01-17 17:25:24.076336\n",
      "Total Rows Read: 42700000, Paired Rows: 3700615, Time: 2018-01-17 17:25:52.883744\n",
      "Total Rows Read: 42800000, Paired Rows: 3709746, Time: 2018-01-17 17:26:22.623617\n",
      "Total Rows Read: 42900000, Paired Rows: 3719095, Time: 2018-01-17 17:26:52.466377\n",
      "Total Rows Read: 43000000, Paired Rows: 3727991, Time: 2018-01-17 17:27:20.726850\n",
      "Cleanin up!\n",
      "Total Rows Read: 43100000, Paired Rows: 3735291, Time: 2018-01-17 17:28:47.144103\n",
      "Total Rows Read: 43200000, Paired Rows: 3743538, Time: 2018-01-17 17:29:16.123521\n",
      "Total Rows Read: 43300000, Paired Rows: 3751549, Time: 2018-01-17 17:29:43.910857\n",
      "Total Rows Read: 43400000, Paired Rows: 3759237, Time: 2018-01-17 17:30:12.477025\n",
      "Total Rows Read: 43500000, Paired Rows: 3767378, Time: 2018-01-17 17:30:40.963322\n",
      "Total Rows Read: 43600000, Paired Rows: 3776571, Time: 2018-01-17 17:31:10.270319\n",
      "Total Rows Read: 43700000, Paired Rows: 3785415, Time: 2018-01-17 17:31:38.320600\n",
      "Total Rows Read: 43800000, Paired Rows: 3793479, Time: 2018-01-17 17:32:05.925304\n",
      "Total Rows Read: 43900000, Paired Rows: 3801958, Time: 2018-01-17 17:32:34.588594\n",
      "Total Rows Read: 44000000, Paired Rows: 3811090, Time: 2018-01-17 17:33:04.896440\n",
      "Cleanin up!\n",
      "Total Rows Read: 44100000, Paired Rows: 3818731, Time: 2018-01-17 17:34:39.385837\n",
      "Total Rows Read: 44200000, Paired Rows: 3827877, Time: 2018-01-17 17:35:08.950075\n",
      "Total Rows Read: 44300000, Paired Rows: 3837428, Time: 2018-01-17 17:35:38.504096\n",
      "Total Rows Read: 44400000, Paired Rows: 3847201, Time: 2018-01-17 17:36:07.519882\n",
      "Total Rows Read: 44500000, Paired Rows: 3857072, Time: 2018-01-17 17:36:35.840003\n",
      "Total Rows Read: 44600000, Paired Rows: 3866925, Time: 2018-01-17 17:37:05.682535\n",
      "Total Rows Read: 44700000, Paired Rows: 3876692, Time: 2018-01-17 17:37:34.671248\n",
      "Total Rows Read: 44800000, Paired Rows: 3886266, Time: 2018-01-17 17:38:05.397046\n",
      "Total Rows Read: 44900000, Paired Rows: 3895600, Time: 2018-01-17 17:38:33.671754\n",
      "Total Rows Read: 45000000, Paired Rows: 3904826, Time: 2018-01-17 17:39:03.393561\n",
      "Cleanin up!\n",
      "Total Rows Read: 45100000, Paired Rows: 3911908, Time: 2018-01-17 17:40:31.930079\n",
      "Total Rows Read: 45200000, Paired Rows: 3920571, Time: 2018-01-17 17:41:00.808121\n",
      "Total Rows Read: 45300000, Paired Rows: 3929172, Time: 2018-01-17 17:41:30.118886\n",
      "Total Rows Read: 45400000, Paired Rows: 3938117, Time: 2018-01-17 17:41:57.384385\n",
      "Total Rows Read: 45500000, Paired Rows: 3946385, Time: 2018-01-17 17:42:26.322604\n",
      "Total Rows Read: 45600000, Paired Rows: 3954477, Time: 2018-01-17 17:42:55.974181\n",
      "Total Rows Read: 45700000, Paired Rows: 3962634, Time: 2018-01-17 17:43:26.274975\n",
      "Total Rows Read: 45800000, Paired Rows: 3971315, Time: 2018-01-17 17:43:55.963455\n",
      "Total Rows Read: 45900000, Paired Rows: 3980507, Time: 2018-01-17 17:44:24.996462\n",
      "Total Rows Read: 46000000, Paired Rows: 3990054, Time: 2018-01-17 17:44:54.681137\n",
      "Cleanin up!\n",
      "Total Rows Read: 46100000, Paired Rows: 3998003, Time: 2018-01-17 17:46:27.040897\n",
      "Total Rows Read: 46200000, Paired Rows: 4007409, Time: 2018-01-17 17:46:57.065428\n",
      "Total Rows Read: 46300000, Paired Rows: 4016871, Time: 2018-01-17 17:47:26.299335\n",
      "Total Rows Read: 46400000, Paired Rows: 4026573, Time: 2018-01-17 17:47:55.794732\n",
      "Total Rows Read: 46500000, Paired Rows: 4036166, Time: 2018-01-17 17:48:25.664346\n",
      "Total Rows Read: 46600000, Paired Rows: 4045261, Time: 2018-01-17 17:48:56.440979\n",
      "Total Rows Read: 46700000, Paired Rows: 4054448, Time: 2018-01-17 17:49:25.040175\n",
      "Total Rows Read: 46800000, Paired Rows: 4063581, Time: 2018-01-17 17:49:54.823290\n",
      "Total Rows Read: 46900000, Paired Rows: 4072657, Time: 2018-01-17 17:50:23.572167\n",
      "Total Rows Read: 47000000, Paired Rows: 4081780, Time: 2018-01-17 17:50:52.078576\n",
      "Cleanin up!\n",
      "Total Rows Read: 47100000, Paired Rows: 4089390, Time: 2018-01-17 17:52:28.256683\n",
      "Total Rows Read: 47200000, Paired Rows: 4097997, Time: 2018-01-17 17:52:57.900646\n",
      "Total Rows Read: 47300000, Paired Rows: 4106598, Time: 2018-01-17 17:53:27.303773\n",
      "Total Rows Read: 47400000, Paired Rows: 4114873, Time: 2018-01-17 17:53:57.152694\n",
      "Total Rows Read: 47500000, Paired Rows: 4122924, Time: 2018-01-17 17:54:26.520951\n",
      "Total Rows Read: 47600000, Paired Rows: 4131387, Time: 2018-01-17 17:54:58.541480\n",
      "Total Rows Read: 47700000, Paired Rows: 4140346, Time: 2018-01-17 17:55:29.785106\n",
      "Total Rows Read: 47800000, Paired Rows: 4149746, Time: 2018-01-17 17:56:00.966116\n",
      "Total Rows Read: 47900000, Paired Rows: 4159294, Time: 2018-01-17 17:56:31.370403\n",
      "Total Rows Read: 48000000, Paired Rows: 4168809, Time: 2018-01-17 17:57:01.289828\n",
      "Cleanin up!\n",
      "Total Rows Read: 48100000, Paired Rows: 4176387, Time: 2018-01-17 17:58:40.457951\n",
      "Total Rows Read: 48200000, Paired Rows: 4185358, Time: 2018-01-17 17:59:12.138142\n",
      "Total Rows Read: 48300000, Paired Rows: 4194867, Time: 2018-01-17 17:59:43.432347\n",
      "Total Rows Read: 48400000, Paired Rows: 4204283, Time: 2018-01-17 18:00:14.260512\n",
      "Total Rows Read: 48500000, Paired Rows: 4213939, Time: 2018-01-17 18:00:45.111789\n",
      "Total Rows Read: 48600000, Paired Rows: 4223478, Time: 2018-01-17 18:01:16.368497\n",
      "Total Rows Read: 48700000, Paired Rows: 4232888, Time: 2018-01-17 18:01:48.145116\n",
      "Total Rows Read: 48800000, Paired Rows: 4241867, Time: 2018-01-17 18:02:18.546391\n",
      "Total Rows Read: 48900000, Paired Rows: 4250661, Time: 2018-01-17 18:02:48.694462\n",
      "Total Rows Read: 49000000, Paired Rows: 4259495, Time: 2018-01-17 18:03:18.560808\n",
      "Cleanin up!\n",
      "Total Rows Read: 49100000, Paired Rows: 4266685, Time: 2018-01-17 18:04:57.318991\n",
      "Total Rows Read: 49200000, Paired Rows: 4274947, Time: 2018-01-17 18:05:27.454569\n",
      "Total Rows Read: 49300000, Paired Rows: 4282950, Time: 2018-01-17 18:05:57.545520\n",
      "Total Rows Read: 49400000, Paired Rows: 4290722, Time: 2018-01-17 18:06:27.671451\n",
      "Total Rows Read: 49500000, Paired Rows: 4299131, Time: 2018-01-17 18:06:57.464752\n",
      "Total Rows Read: 49600000, Paired Rows: 4308200, Time: 2018-01-17 18:07:28.143685\n",
      "Total Rows Read: 49700000, Paired Rows: 4317521, Time: 2018-01-17 18:07:58.130275\n",
      "Total Rows Read: 49800000, Paired Rows: 4327359, Time: 2018-01-17 18:08:27.748449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Read: 49900000, Paired Rows: 4337458, Time: 2018-01-17 18:08:57.510569\n",
      "Total Rows Read: 50000000, Paired Rows: 4347385, Time: 2018-01-17 18:09:27.731088\n",
      "Cleanin up!\n",
      "Total Rows Read: 50100000, Paired Rows: 4355460, Time: 2018-01-17 18:11:10.872509\n",
      "Total Rows Read: 50200000, Paired Rows: 4364895, Time: 2018-01-17 18:11:43.092542\n",
      "Total Rows Read: 50300000, Paired Rows: 4374802, Time: 2018-01-17 18:12:13.331102\n",
      "Total Rows Read: 50400000, Paired Rows: 4384265, Time: 2018-01-17 18:12:43.422418\n",
      "Total Rows Read: 50500000, Paired Rows: 4393840, Time: 2018-01-17 18:13:13.548588\n",
      "Total Rows Read: 50600000, Paired Rows: 4402968, Time: 2018-01-17 18:13:42.771716\n",
      "Total Rows Read: 50700000, Paired Rows: 4411981, Time: 2018-01-17 18:14:13.362115\n",
      "Total Rows Read: 50800000, Paired Rows: 4420979, Time: 2018-01-17 18:14:41.632228\n",
      "Total Rows Read: 50900000, Paired Rows: 4430259, Time: 2018-01-17 18:15:10.519969\n",
      "Total Rows Read: 51000000, Paired Rows: 4439093, Time: 2018-01-17 18:15:39.786240\n",
      "Cleanin up!\n",
      "Total Rows Read: 51100000, Paired Rows: 4445882, Time: 2018-01-17 18:17:20.179631\n",
      "Total Rows Read: 51200000, Paired Rows: 4453244, Time: 2018-01-17 18:17:51.025735\n",
      "Total Rows Read: 51300000, Paired Rows: 4461344, Time: 2018-01-17 18:18:21.847274\n",
      "Total Rows Read: 51400000, Paired Rows: 4470218, Time: 2018-01-17 18:18:52.660275\n",
      "Total Rows Read: 51500000, Paired Rows: 4479356, Time: 2018-01-17 18:19:23.473003\n",
      "Total Rows Read: 51600000, Paired Rows: 4488847, Time: 2018-01-17 18:19:52.383734\n",
      "Total Rows Read: 51700000, Paired Rows: 4498473, Time: 2018-01-17 18:20:21.376842\n",
      "Total Rows Read: 51800000, Paired Rows: 4508291, Time: 2018-01-17 18:20:49.974608\n",
      "Total Rows Read: 51900000, Paired Rows: 4518003, Time: 2018-01-17 18:21:19.622179\n",
      "Total Rows Read: 52000000, Paired Rows: 4527719, Time: 2018-01-17 18:21:49.373431\n",
      "Cleanin up!\n",
      "Total Rows Read: 52100000, Paired Rows: 4535222, Time: 2018-01-17 18:23:27.069366\n",
      "Total Rows Read: 52200000, Paired Rows: 4544150, Time: 2018-01-17 18:23:57.391668\n",
      "Total Rows Read: 52300000, Paired Rows: 4553284, Time: 2018-01-17 18:24:28.115733\n",
      "Total Rows Read: 52400000, Paired Rows: 4562307, Time: 2018-01-17 18:24:58.428495\n",
      "Total Rows Read: 52500000, Paired Rows: 4570970, Time: 2018-01-17 18:25:27.522620\n",
      "Total Rows Read: 52600000, Paired Rows: 4579757, Time: 2018-01-17 18:25:57.181180\n",
      "Total Rows Read: 52700000, Paired Rows: 4588387, Time: 2018-01-17 18:26:25.689219\n",
      "Total Rows Read: 52800000, Paired Rows: 4597001, Time: 2018-01-17 18:26:55.140946\n",
      "Total Rows Read: 52900000, Paired Rows: 4605271, Time: 2018-01-17 18:27:24.788601\n",
      "Total Rows Read: 53000000, Paired Rows: 4612907, Time: 2018-01-17 18:27:54.348825\n",
      "Cleanin up!\n",
      "Total Rows Read: 53100000, Paired Rows: 4619497, Time: 2018-01-17 18:29:38.080418\n",
      "Total Rows Read: 53200000, Paired Rows: 4627464, Time: 2018-01-17 18:30:06.806623\n",
      "Total Rows Read: 53300000, Paired Rows: 4636140, Time: 2018-01-17 18:30:34.756821\n",
      "Total Rows Read: 53400000, Paired Rows: 4645211, Time: 2018-01-17 18:31:02.363412\n",
      "Total Rows Read: 53500000, Paired Rows: 4654314, Time: 2018-01-17 18:31:30.899962\n",
      "Total Rows Read: 53600000, Paired Rows: 4663417, Time: 2018-01-17 18:31:59.424346\n",
      "Total Rows Read: 53700000, Paired Rows: 4672647, Time: 2018-01-17 18:32:27.705916\n",
      "Total Rows Read: 53800000, Paired Rows: 4681653, Time: 2018-01-17 18:32:55.488533\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_table()\n",
    "    row_counter = 0\n",
    "    paired_rows = 0\n",
    "\n",
    "    #with open('J:/chatdata/reddit_data/{}/RC_{}'.format(timeframe.split('-')[0],timeframe), buffering=1000) as f:\n",
    "    with open('D:\\DEAN\\CHATBOTS\\RC_2015-01'.format(timeframe), buffering=1000) as f:\n",
    "        for row in f:\n",
    "            #print(row)\n",
    "            #time.sleep(555)\n",
    "            row_counter += 1\n",
    "\n",
    "            if row_counter > start_row:\n",
    "                try:\n",
    "                    row = json.loads(row)\n",
    "                    parent_id = row['parent_id'].split('_')[1]\n",
    "                    body = format_data(row['body'])\n",
    "                    created_utc = row['created_utc']\n",
    "                    score = row['score']\n",
    "                    \n",
    "                    comment_id = row['id']\n",
    "                    \n",
    "                    subreddit = row['subreddit']\n",
    "                    parent_data = find_parent(parent_id)\n",
    "                    \n",
    "                    existing_comment_score = find_existing_score(parent_id)\n",
    "                    if existing_comment_score:\n",
    "                        if score > existing_comment_score:\n",
    "                            if acceptable(body):\n",
    "                                sql_insert_replace_comment(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                                \n",
    "                    else:\n",
    "                        if acceptable(body):\n",
    "                            if parent_data:\n",
    "                                if score >= 2:\n",
    "                                    sql_insert_has_parent(comment_id,parent_id,parent_data,body,subreddit,created_utc,score)\n",
    "                                    paired_rows += 1\n",
    "                            else:\n",
    "                                sql_insert_no_parent(comment_id,parent_id,body,subreddit,created_utc,score)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                            \n",
    "            if row_counter % 100000 == 0:\n",
    "                print('Total Rows Read: {}, Paired Rows: {}, Time: {}'.format(row_counter, paired_rows, str(datetime.now())))\n",
    "\n",
    "            if row_counter > start_row:\n",
    "                if row_counter % cleanup == 0:\n",
    "                    print(\"Cleanin up!\")\n",
    "                    sql = \"DELETE FROM parent_reply WHERE parent IS NULL\"\n",
    "                    c.execute(sql)\n",
    "                    connection.commit()\n",
    "                    c.execute(\"VACUUM\")\n",
    "                    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframes = ['2015-05']\n",
    "\n",
    "for timeframe in timeframes:\n",
    "    connection = sqlite3.connect('{}.db'.format(timeframe))\n",
    "    c = connection.cursor()\n",
    "    limit = 5000\n",
    "    last_unix = 0\n",
    "    cur_length = limit\n",
    "    counter = 0\n",
    "    test_done = False while cur_length == limit:\n",
    "\n",
    "        df = pd.read_sql(\"SELECT * FROM parent_reply WHERE unix > {} and parent NOT NULL and score > 0 ORDER BY unix ASC LIMIT {}\".format(last_unix,limit),connection)\n",
    "        last_unix = df.tail(1)['unix'].values[0]\n",
    "        cur_length = len(df)\n",
    "\n",
    "        if not test_done:\n",
    "            with open('test.from','a', encoding='utf8') as f:\n",
    "                for content in df['parent'].values:\n",
    "                    f.write(content+'\\n')\n",
    "\n",
    "            with open('test.to','a', encoding='utf8') as f:\n",
    "                for content in df['comment'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "\n",
    "            test_done = True\n",
    "\n",
    "        else:\n",
    "            with open('train.from','a', encoding='utf8') as f:\n",
    "                for content in df['parent'].values:\n",
    "                    f.write(content+'\\n')\n",
    "\n",
    "            with open('train.to','a', encoding='utf8') as f:\n",
    "                for content in df['comment'].values:\n",
    "                    f.write(str(content)+'\\n')\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 20 == 0:\n",
    "            print(counter*limit,'rows completed so far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    '''Create the encoding layer'''\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                   cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length,\n",
    "                                                   inputs = rnn_inputs, \n",
    "                                                   dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    '''Decode the training data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                     att_keys,\n",
    "                                                                     att_vals,\n",
    "                                                                     att_score_fn,\n",
    "                                                                     att_construct_fn,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input, \n",
    "                                                              sequence_length, \n",
    "                                                              scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    '''Decode the prediction data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         start_of_sequence_id, \n",
    "                                                                         end_of_sequence_id, \n",
    "                                                                         maximum_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    '''Create the decoding cell and input the parameters for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope,\n",
    "                                                                weights_initializer = weights,\n",
    "                                                                biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, keep_prob, \n",
    "                                            batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                       answers_vocab_size+1, \n",
    "                                                       enc_embedding_size,\n",
    "                                                       initializer = tf.random_uniform_initializer(0,1))\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state, \n",
    "                                                questions_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                questions_vocab_to_int, \n",
    "                                                keep_prob, \n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "# Start the session\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# Load the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "# Find the shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), \n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "    questions_vocab_to_int)\n",
    "\n",
    "# Create a tensor for the inference logits, needed if loading a checkpoint version of the model\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the training with 10% of the data\n",
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "# Split the questions and answers into training and validating data\n",
    "train_questions = sorted_questions[train_valid_split:]\n",
    "train_answers = sorted_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = sorted_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 100 # Check training loss after every 100 batches\n",
    "stop_early = 0 \n",
    "stop = 5 # If the validation loss does decrease in 5 consecutive checks, stop training\n",
    "validation_check = ((len(train_questions))//batch_size//2)-1 # Modulus for checking validation loss\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch_i in range(1, epochs+1):\n",
    "    for batch_i, (questions_batch, answers_batch) in enumerate(\n",
    "            batch_data(train_questions, train_answers, batch_size)):\n",
    "        start_time = time.time()\n",
    "        _, loss = sess.run(\n",
    "            [train_op, cost],\n",
    "            {input_data: questions_batch,\n",
    "             targets: answers_batch,\n",
    "             lr: learning_rate,\n",
    "             sequence_length: answers_batch.shape[1],\n",
    "             keep_prob: keep_probability})\n",
    "\n",
    "        total_train_loss += loss\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "\n",
    "        if batch_i % display_step == 0:\n",
    "            print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                  .format(epoch_i,\n",
    "                          epochs, \n",
    "                          batch_i, \n",
    "                          len(train_questions) // batch_size, \n",
    "                          total_train_loss / display_step, \n",
    "                          batch_time*display_step))\n",
    "            total_train_loss = 0\n",
    "\n",
    "        if batch_i % validation_check == 0 and batch_i > 0:\n",
    "            total_valid_loss = 0\n",
    "            start_time = time.time()\n",
    "            for batch_ii, (questions_batch, answers_batch) in \\\n",
    "                    enumerate(batch_data(valid_questions, valid_answers, batch_size)):\n",
    "                valid_loss = sess.run(\n",
    "                cost, {input_data: questions_batch,\n",
    "                       targets: answers_batch,\n",
    "                       lr: learning_rate,\n",
    "                       sequence_length: answers_batch.shape[1],\n",
    "                       keep_prob: 1})\n",
    "                total_valid_loss += valid_loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            avg_valid_loss = total_valid_loss / (len(valid_questions) / batch_size)\n",
    "            print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "            \n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "\n",
    "            summary_valid_loss.append(avg_valid_loss)\n",
    "            if avg_valid_loss <= min(summary_valid_loss):\n",
    "                print('New Record!') \n",
    "                stop_early = 0\n",
    "                saver = tf.train.Saver() \n",
    "                saver.save(sess, checkpoint)\n",
    "\n",
    "            else:\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == stop:\n",
    "                    break\n",
    "    \n",
    "    if stop_early == stop:\n",
    "        print(\"Stopping Training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int):\n",
    "    '''Prepare the question for the model'''\n",
    "    \n",
    "    question = clean_text(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own input question\n",
    "#input_question = 'How are you?'\n",
    "\n",
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(short_questions))\n",
    "input_question = short_questions[random]\n",
    "\n",
    "# Prepare the question\n",
    "input_question = question_to_seq(input_question, questions_vocab_to_int)\n",
    "\n",
    "# Pad the questions until it equals the max_line_length\n",
    "input_question = input_question + [questions_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_question))\n",
    "# Add empty questions so the the input_data is the correct shape\n",
    "batch_shell = np.zeros((batch_size, max_line_length))\n",
    "# Set the first question to be out input question\n",
    "batch_shell[0] = input_question    \n",
    "    \n",
    "# Run the model with the input question\n",
    "answer_logits = sess.run(inference_logits, {input_data: batch_shell, \n",
    "                                            keep_prob: 1.0})[0]\n",
    "\n",
    "# Remove the padding from the Question and Answer\n",
    "pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "pad_a = answers_vocab_to_int[\"<PAD>\"]\n",
    "\n",
    "print('Question')\n",
    "print('  Word Ids:      {}'.format([i for i in input_question if i != pad_q]))\n",
    "print('  Input Words: {}'.format([questions_int_to_vocab[i] for i in input_question if i != pad_q]))\n",
    "\n",
    "print('\\nAnswer')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(answer_logits, 1) if i != pad_a]))\n",
    "print('  Response Words: {}'.format([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
